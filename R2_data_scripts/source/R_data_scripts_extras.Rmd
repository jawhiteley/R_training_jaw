---
title: "A Short Introduction to Working With Data in R"
subtitle: 'EXTRAS'
author: "Jonathan Whiteley"
date: "`r Sys.Date()`"
output: 
  beamer_presentation: 
    latex_engine: xelatex # ensures certain characters (like '^') are rendered correctly
    theme: Boadilla
    colortheme: default
    highlight: ../../shared/my_tango.theme
    slide_level: 2
    toc: true
    includes:
      in_header: "../../shared/preamble-include.tex"
fontsize: 11pt
urlcolor: darkblue   # defined in LaTeX preamble (in_header includes, above)
linkcolor: darkblue  # defined in LaTeX preamble (in_header includes, above)
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "..") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment='#')
options(width = 60)  # to fit output on slides
```


# Reading a csv file with base $\R$

## Load a csv file using `read.csv()`

```r
?read.csv
```
- If `read.csv()`{.r} encounters problems reading a file,
  it is more likely to trigger an **error** than `read_csv()`{.r},
  which gives a warning more often.
  
```{r load with read_csv (1st try), message=FALSE, warning=TRUE}
DF_path <- file.path("..", "data", "data_example.csv")
try( read.csv(DF_path) )
```

## Check the file contents

- Let's take a peek at the first few lines and see if we can identify the problem
  (this is more often necessary with `read.csv()`{.r}):

```{r readlines}
readLines(DF_path, n = 4)
```

- The first **2** lines don't look like comma-separated values!

- They look like extra information that is not part of the data table *structure*.


## Load a csv file into $\R$

- We can tell $\R$ to skip the lines with no data:

```{r load, message=FALSE}
DF <- read.csv(DF_path, skip = 2)
DF_readr <- readr::read_csv(DF_path, skip = 2)
```

- Just because there were no `Error`{.r}s from $\R$, 
  doesn't mean there's nothing wrong with the data!


## `readr` vs base $\R$ functions

+--------------------+-----------------------------+--------------------+-----------------------+
| **`readr`**                                      | **`base R`**                               |
+====================+=============================+====================+=======================+
| `read_csv()`{.r}   | comma separated values      | `read.csv()`{.r}   |                       |
+--------------------+-----------------------------+--------------------+-----------------------+
| `read_csv2()`{.r}  | '`;`' as delimiter\         | `read.csv2()`{.r}  | '`,`' for decimals,\  |
|                    | (allows '`,`' for decimals) |                    | '`;`' as separator    |
+--------------------+-----------------------------+--------------------+-----------------------+
| `read_tsv()`{.r}   | tab separated values        | `read.delim()`{.r} | delimited files       |
|                    |                             |                    | (tab is default)      |
+--------------------+-----------------------------+--------------------+-----------------------+
| `read_delim()`{.r} | (generic) files             | `read.table()`{.r} |                       |
|                    | with any delimiter          |                    |                       |
+--------------------+-----------------------------+--------------------+-----------------------+
| `read_fwf()`{.r}   | fixed width files           | `read.fwf()`{.r}   |                       |
+--------------------+-----------------------------+--------------------+-----------------------+

`readr` descriptions based on [#dsbox](https://datasciencebox.org/course-materials/_slides/u2-d12-data-import/u2-d12-data-import#5)


## Comparison of `read.csv()` and `read_csv()`

- In keeping with Tidyverse conventions, functions are names with words separated by "`_`" 
  - instead of "`.`" or `camelCase`, as in many base $\R$ functions

- The column names are different
  - `read.csv()`{.r} automatically applies `make.names()`{.r} to the column names 
    to make 'syntactically valid' names to use in $\R$.
  - convenient, but not always what we want.
  - there are other 'cleaning' functions available 
    (e.g.,\ `clean_names()`{.r} in the [`janitor` package](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html))

- `read_csv()`{.r} automatically replaced empty strings 
  in the `Treatment` column with `NA`{.r}s.

- `read_csv()`{.r} left the '`675`' column as numeric,
  but ignored the commas, resulting in larger numbers.

- `read_csv()`{.r} produces a `"tbl_df"` (*tibble*) object,
  not a simple `data.frame`


## Tibble examples {.shrink}

- Tibbles have an enhanced `print()`{.r} method
- and they will _not_ do partial matching on variable names,
  triggering a *warning* instead for columns that do not exist.
  
```{r tibble examples}
print(DF_readr, n=2)
is.null(DF$Treat)
is.null(DF_readr$Treat)
```



# File Encoding, Windows, & Microsoft Excel^TM^

## Encoding non-English characters {.shrink}

```{r reproduce windows latin1, include=FALSE, results='hide', purl = FALSE}
## Reproduce Windows encoding (bad)
DF_win <- read.csv(DF_path, skip = 2, encoding = "latin1")
## Extract example of messed up encoding
latin1_ex <- DF_win |>
  dplyr::select(Type) |>
  dplyr::filter( ! Type %in% c("Quebec", "Mississippi") ) |> 
  unlist() |> unique()
```

- If you are running $\R$ in Windows, you may notice that 
  some text values (character) look strange when read
  with `read.csv()`{.r}:
  
  "**`r latin1_ex`**" instead of "Québec"

- There is nothing wrong with the file 
  --- this indicates a _mismatch_ between 
  the *encoding* used to write the file,
  and what $\R$ used to read it.

- Even though '`.csv`' files are plain text,
  letters (especially non-english characters)
  can be *encoded* in different ways
  to represent them in the computer.
  
- "[`UTF-8`](https://en.wikipedia.org/wiki/UTF-8)" 
  is a character encoding standard designed to handle many non-english characters.

  - The example data file was written in "`UTF-8`"
  - Most OSes and many programs use "`UTF-8`" encoding by default.
  - But *Windows* uses "`latin1`" by default,
    and so does $\R$ (< [4.2.0](https://stat.ethz.ch/pipermail/r-announce/2022/000683.html)) when running in Windows.
  - Starting with [v4.2.0](https://stat.ethz.ch/pipermail/r-announce/2022/000683.html), $\R$ uses "`UTF-8`" as the default encoding on Windows.


## Read a csv file with a different encoding {.shrink}

- You can specify the encoding used in the file 
  with the '`encoding`' argument of `read.csv()`{.r}
  
```{r read.csv() with encoding}
DF <- read.csv(DF_path, skip = 2, encoding = "UTF-8")
```

- If reading a file that was 
  created on a Windows computer and encoded in "`latin1`",
  on a different system (mac, Unix, linux, etc.)
  --- or a recent version of $\R$ (>=4.2) on Windows ---
  you can specify that, too:
  
``` r
read.csv(DF_path, skip = 2, encoding = "latin1")
```


## Encoding & Microsoft Excel^TM^

- Excel can save a `.csv` file using `UTF-8` encoding,
  but in doing so, it adds "[**b**yte **o**rder **m**ark](https://en.wikipedia.org/wiki/Byte_order_mark)" ("BOM")
  to the file.
  - This is a special character that Excel also uses to recognize
    that the file is encoded using `UTF-8`.
  - Thus a BOM can make the file "easier" to use with Excel,
    by allowing it to automatically recognize the `UTF-8` encoding,
    but it can also **cause problems for other programs** (like $\R$) 
    that do not expect such a non-Unicode character.
  
- Without the BOM, Excel will assume the file is encoded in "`latin1`"
  if you double-click on the csv file to open it in Excel,
  even if it was actually encoded with `UTF-8`.
  - This can cause special characters to appear incorrectly.
  - You can still import a `.csv` file encoded in `UTF-8`
    into Excel correctly, but it requires opening the file within Excel,
    or [importing it using commands in the "Data" ribbon / menu](https://stackoverflow.com/questions/6002256/is-it-possible-to-force-excel-recognize-utf-8-csv-files-automatically)


## Read a file with a BOM using `read.csv()` in $\R$

- Reading a `.csv` file with a BOM using the usual method 
  may cause [the BOM to be included in the name of the first column](https://stackoverflow.com/questions/39593637/dealing-with-byte-order-mark-bom-in-r)
  (on Windows).
  
```{r read.csv() BOM, results='hide'}
bom_bad <- read.csv("../data/data_example_bom.csv", 
                    encoding = "UTF-8")
names(bom_bad)
```

- The solution with `read.csv()`{.r} is to use the argument  
  '`fileEncoding = "UTF-8-BOM"`' 
  (instead of the '`encoding`' argument)
  
```{r read.csv() BOM fileEncoding}
bom <- read.csv("../data/data_example_bom.csv", 
                fileEncoding = "UTF-8-BOM")
bom[4, 1:4]
```

::: notes
?read.csv
* encoding = specifies how the text in the input file is encoded
  - it is *not* used to re-encode the input, but to handle it in its native encoding
* fileEncoding = tells R the encoding used in the file; 
  - R will re-encode it into the native encoding.
- i.e., `fileEncoding = "UTF-8-BOM" tells R to re-encode the input from UTF-8 (and drop the BOM)
  - `encoding` alone might not drop the BOM, because there is no re-encoding.
:::


## Read a file with a BOM using `read_csv()` {.shrink}

- The `readr` package uses "`UTF-8`" encoding by default,
  and **automatically ignores a BOM**, if present.

```{r read_csv() BOM, message=FALSE}
bom_readr <- readr::read_csv("../data/data_example_bom.csv")
bom_readr[4, 1:4] |> knitr::kable()
```

- `write_csv()` (in the `readr` package) automatically encodes output files using "UTF-8",
  for greater portability across systems.
  - _except_ for older versions of base $\R$ (`read.csv()`) on Windows :(

### !

Hopefully, these examples have demonstrated that 
the `readr` package makes it easy to 
work with "`UTF-8`" files by default, on any platform.

::: notes
Load a file with a BOM 
- https://stackoverflow.com/questions/39593637/dealing-with-byte-order-mark-bom-in-r
- https://github.com/ropensci-archive/gtfsr/issues/19#issuecomment-247766324
- https://github.com/tidyverse/readr/issues/263
- https://github.com/tidyverse/readr/issues/500
- https://stackoverflow.com/questions/18789330/r-on-windows-character-encoding-hell

Change default encoding for R:
- https://stackoverflow.com/questions/31308541/r-default-encoding-to-utf-8
:::


## Add a BOM to an output file {.shrink}

<!--
- Some programs (e.g., Excel) expect a BOM
  in "`UTF-8`" encoded files.
  
  - Excel will automatically use `UTF-8` when opening a file with a BOM.
-->

- It is possible to add a BOM to a csv file,
  but it must be done _manually_ with base $\R$:
  - code adapted from [this StackOverflow answer](https://stackoverflow.com/a/41408091)
  
```r
writeChar(
  iconv("\ufeff", to = "UTF-8"), 
  "output.csv", 
  eos = NULL
)
write.csv(Data, "output.csv", append = TRUE, ... )
```

- The `readr` package can do this directly 
  with a special `write_excel_csv`{.r} function:

```r
write_excel_csv(Data, "output.csv", ... )
```

### !

**$\R$ does not recommend doing this** (see `?file`),
  so use with caution.

::: notes
Add a BOM to a file
- ?file
- https://stackoverflow.com/a/41408091
:::


## Using other encodings with `readr`

- You can control the encoding used by `readr` functions with the **`locale` argument**.

```r
write_csv(Data, "data.csv", 
          locale = locale(encoding = "latin1")
)

read_csv(Data, "data.csv", 
         locale = locale(encoding = "latin1")
)
```

- See `?readr::read_csv` and `?readr::locale` for details.




# Downloading Data From the Internet

```{=html}
<!--
- You can use read.csv & read_csv on urls directly!
  - the same example file, directly from the URL on GitHub
- downloading data from the internet (to a temp dir)
  - the same example file, directly from the URL on GitHub
  - download.file()
- CANSIM / NDMr / other package?
-->
```




# Reading Data in Other Formats

- Parquet files: an efficient columnar format,
  popular with Big Data and cloud computing
  
  - [Apache Arrow](https://github.com/apache/arrow/tree/main/r) 
    (i.e., the '`arrow`' package)
  
- The [Data Import Chapter of R for Data Science](https://r4ds.had.co.nz/data-import.html#other-types-of-data) ([2e](https://r4ds.hadley.nz/data-import)) 
  describes these tidyverse packages for other types of data:

  - **`haven`** reads SPSS, Stata, and SAS files.
  
  - **`DBI`**, along with a database specific backend (e.g. `RMySQL`, `RSQLite`, `RPostgreSQL`, etc.) 
    allows you to run SQL queries on a **database** and return a data frame.
  
- See the [Import Section](https://r4ds.hadley.nz/import) of [R for Data Science (2nd edition)](https://r4ds.hadley.nz/) 
  For more details on getting data into R from these and other sources.
    
- Other options are also described in the [R Data Import/Export](https://cran.r-project.org/doc/manuals/r-release/R-data.html) manual.




# Exporting to other formats

## Writing to Microsoft Excel^TM^ files {.shrink}

Packages that can write to Excel files:

-   [`xlsx`](https://github.com/colearendt/xlsx): read, write, format Excel 2007 (`.xlsx`) and Excel 97/2000/XP/2003 (`.xls`) files.
    -   Requires Java and the `rJava` package
-   [`XLConnect`](https://github.com/miraisolutions/xlconnect): comprehensive and cross-platform R package for manipulating Microsoft Excel files (`.xlsx` & `.xls`) from within R.
    -   Requires a Java Runtime Environment (JRE)
-   [`openxlsx`](https://ycphs.github.io/openxlsx/index.html): simplified creation of Excel `.xlsx` files (**not** `.xls`).
    -   *No dependency* on Java
-   [`writexl`](https://docs.ropensci.org/writexl/): portable, light-weight data frame to **xlsx** exporter.
    -   No Java or Excel required

### !

I recommend *avoiding* exporting data to Excel files if possible. `csv` files are easier to read to & write from, and can be read by a wider variety of software (they are more portable).  
Automated reports can be produced with R Markdown and output to a variety of more portable formats (pdf, HTML, etc.) instead.

::: notes
As seen here, there are several packages for working with Excel files, with different advantages and disadvantages: you may have to do some testing to find the best fit for your situation.

I personally prefer the `openxlsx` package, because in my experience, the dependency on Java can be difficult to get working and unreliable. But I have no experience with `writexl`, which is newer and possibly faster than the others.\
The main trade-off is the openxlsx cannot write to `.xls` files (Excel 97/2000/XP/2003 format) and has fewer formatting options than other packages --- but if you need that much formatting, why not produce a formatted report with R Markdown?
:::

## References (Extras)

CANSIM / CODR data:

- [An ecosystem of R packages to access and process Canadian data](https://mountainmath.ca/canssi/#1)
- [Analyzing Canadian Demographic and Housing Data](https://mountainmath.github.io/canadian_data/)
